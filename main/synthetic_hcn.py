# -*- coding: utf-8 -*-
"""Synthetic HCN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wd5G9SBmILP_dvLrXs6TCTT_onS2UNDr
"""

!pip install sdv ctgan --quiet

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# pip install sdv==1.13.1 ctgan==0.7.4 pandas numpy matplotlib scipy

import os, json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import spearmanr, kendalltau
from sdv.single_table import CTGANSynthesizer
from sdv.metadata import SingleTableMetadata

OUT = "./out_ctgan_trust"
os.makedirs(OUT, exist_ok=True)

# -----------------------------
# Utilities
# -----------------------------
def row_stochastic(M, eps=1e-12):
    M = np.maximum(M, 0.0)
    rs = M.sum(axis=1, keepdims=True)
    rs = np.where(rs < eps, 1.0, rs)
    return M / rs

def normalize(v):
    v = np.asarray(v, dtype=float)
    mn, mx = np.min(v), np.max(v)
    if mx - mn <= 1e-12:
        return np.zeros_like(v)
    return (v - mn) / (mx - mn)

def metrics(y_true, y_score, ks):
    r = normalize(y_true); s = normalize(y_score)
    rmse = float(np.sqrt(np.mean((s - r)**2)))
    mae  = float(np.mean(np.abs(s - r)))
    rho, _ = spearmanr(r, s)
    tau, _ = kendalltau(r, s)
    gt_rank = np.argsort(-r); pr_rank = np.argsort(-s)
    rows = []
    for k in ks:
        gt_top = set(gt_rank[:k]); pr_top = set(pr_rank[:k])
        tp = len(gt_top & pr_top)
        prec = tp / k; rec = tp / k
        f1 = 0.0 if prec+rec==0 else 2*prec*rec/(prec+rec)
        rows.append((k, prec, rec, f1, rmse, mae, rho, tau, len(r)))
    return rows

def social_score_iter(tau_intra, delta_alpha, tau_ba=None, delta_beta=None, max_iter=200, tol=1e-4):
    S = delta_alpha.copy()
    if (tau_ba is not None) and (delta_beta is not None):
        S = S + delta_beta @ tau_ba
    S_prev = np.zeros_like(S)
    it = 0
    while it < max_iter and np.linalg.norm(S - S_prev, ord=1) > tol:
        S_prev = S.copy()
        S = S @ tau_intra
        it += 1
    return S.flatten()

def to_edge_table(tau, layer, src_prefix, dst_prefix):
    n_src, n_dst = tau.shape
    data = []
    for i in range(n_src):
        for j in range(n_dst):
            data.append([layer, f"{src_prefix}{i}", f"{dst_prefix}{j}", float(tau[i,j])])
    return pd.DataFrame(data, columns=["layer","src","dst","trust"])

def filter_layer(df, name, src_prefix=None, dst_prefix=None):
    df_f = df[df["layer"] == name].copy()
    if src_prefix:
        df_f = df_f[df_f["src"].str.startswith(src_prefix)]
    if dst_prefix:
        df_f = df_f[df_f["dst"].str.startswith(dst_prefix)]
    return df_f

# --- FIX: pass original IDs to keep shape ---
def rebuild_tau(df_layer, src_prefix, dst_prefix, all_src_ids=None, all_dst_ids=None):
    df_layer = df_layer[df_layer["src"].str.startswith(src_prefix)]
    df_layer = df_layer[df_layer["dst"].str.startswith(dst_prefix)]
    if all_src_ids is None:
        all_src_ids = sorted(df_layer["src"].unique(), key=lambda x: int(x.replace(src_prefix,"")))
    if all_dst_ids is None:
        all_dst_ids = sorted(df_layer["dst"].unique(), key=lambda x: int(x.replace(dst_prefix,"")))
    piv = df_layer.pivot_table(index="src", columns="dst", values="trust", aggfunc="mean", fill_value=0.0)
    piv = piv.reindex(index=all_src_ids, columns=all_dst_ids, fill_value=0.0)
    return row_stochastic(piv.values.astype(float)), all_src_ids, all_dst_ids

def plot_hist(arr, title, xlabel, path):
    plt.figure()
    plt.hist(arr, bins=20)
    plt.title(title); plt.xlabel(xlabel); plt.ylabel("Frequency")
    plt.tight_layout(); plt.savefig(path, dpi=200); plt.close()

def plot_scatter(x, y, title, xlabel, ylabel, path):
    plt.figure()
    plt.scatter(x, y, s=16)
    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)
    lo = min(np.min(x), np.min(y)); hi = max(np.max(x), np.max(y))
    plt.plot([lo, hi], [lo, hi])
    plt.tight_layout(); plt.savefig(path, dpi=200); plt.close()

def plot_matrix(M, title, path):
    plt.figure()
    plt.imshow(M, aspect='auto')
    plt.title(title); plt.colorbar()
    plt.tight_layout(); plt.savefig(path, dpi=200); plt.close()

# -----------------------------
# 1) Example matrices (replace with extracted real τ)
# -----------------------------
tau_h  = np.array([[0,0.5,0.25,0.25],[0.5,0,0.25,0.25],[0.33,0.33,0,0.33],[0.33,0.33,0.33,0]], dtype=float)
tau_d  = np.array([[0,0.5,0.5,0],[1,0,0,0],[0.5,0,0,0.5],[0,0,1,0]], dtype=float)
tau_p  = np.array([[0,0.17,0.33,0.33,0.16],[0.33,0,0.33,0.33,0],[0.33,0.17,0,0.33,0.16],[0.33,0.17,0.33,0,0.16],[0.33,0,0.33,0.33,0]], dtype=float)

tau_hd = np.array([[0.5,0.25,0.25,0],[0.33,0.67,0,0],[0.67,0,0,0.33],[1,0,0,0]], dtype=float)
tau_dh = np.array([[0.33,0.17,0.33,0.17],[0.33,0.67,0,0],[1,0,0,0],[0,0,1,0]], dtype=float)
tau_dp = np.array([[0.42,0.25,0.33,0,0],[0,0,0.4,0.6,0],[0,0.67,0,0,0.33],[0,0,0,0,1]], dtype=float)
tau_pd = np.array([[1,0,0,0],[0.43,0,0.57,0],[0.67,0.33,0,0],[0,1,0,0],[0,0,0.40,0.60]], dtype=float)

Delta_h = np.full((1, tau_h.shape[0]), 0.2)
Delta_d = np.full((1, tau_d.shape[0]), 0.2)
Delta_p = np.full((1, tau_p.shape[0]), 0.2)

# IDs for reindexing synthetic τ
ids_h = [f"h{i}" for i in range(tau_h.shape[0])]
ids_d = [f"d{i}" for i in range(tau_d.shape[0])]
ids_p = [f"p{i}" for i in range(tau_p.shape[0])]

# -----------------------------
# 2) Build CTGAN training table
# -----------------------------
edges = []
edges.append(to_edge_table(tau_h, "H_intra", "h", "h"))
edges.append(to_edge_table(tau_d, "D_intra", "d", "d"))
edges.append(to_edge_table(tau_p, "P_intra", "p", "p"))
edges.append(to_edge_table(tau_hd, "HD", "h", "d"))
edges.append(to_edge_table(tau_dh, "DH", "d", "h"))
edges.append(to_edge_table(tau_dp, "DP", "d", "p"))
edges.append(to_edge_table(tau_pd, "PD", "p", "d"))
train_df = pd.concat(edges, ignore_index=True)

meta = SingleTableMetadata()
meta.detect_from_dataframe(train_df)
meta.update_column("layer", sdtype="categorical")
meta.update_column("src", sdtype="categorical")
meta.update_column("dst", sdtype="categorical")
meta.update_column("trust", sdtype="numerical")

# -----------------------------
# 3) Fit CTGAN (batch size multiple of pac=1)
# -----------------------------
ctgan = CTGANSynthesizer(metadata=meta, epochs=300, batch_size=64, pac=1)
ctgan.fit(train_df)

# -----------------------------
# 4) Sample synthetic edges & rebuild τ (fixed shape)
# -----------------------------
synth_df = ctgan.sample(num_rows=len(train_df))

tau_h_syn, _, _  = rebuild_tau(filter_layer(synth_df,"H_intra","h","h"), "h", "h", ids_h, ids_h)
tau_d_syn, _, _  = rebuild_tau(filter_layer(synth_df,"D_intra","d","d"), "d", "d", ids_d, ids_d)
tau_p_syn, _, _  = rebuild_tau(filter_layer(synth_df,"P_intra","p","p"), "p", "p", ids_p, ids_p)
tau_hd_syn,_,_   = rebuild_tau(filter_layer(synth_df,"HD","h","d"), "h", "d", ids_h, ids_d)
tau_dh_syn,_,_   = rebuild_tau(filter_layer(synth_df,"DH","d","h"), "d", "h", ids_d, ids_h)
tau_dp_syn,_,_   = rebuild_tau(filter_layer(synth_df,"DP","d","p"), "d", "p", ids_d, ids_p)
tau_pd_syn,_,_   = rebuild_tau(filter_layer(synth_df,"PD","p","d"), "p", "d", ids_p, ids_d)

# -----------------------------
# 5) Social scores: real vs synthetic
# -----------------------------
S_h_real = social_score_iter(tau_h, Delta_h, tau_ba=tau_dh, delta_beta=Delta_d)
S_d_real = social_score_iter(tau_d, Delta_d, tau_ba=tau_pd, delta_beta=Delta_p)
S_p_real = social_score_iter(tau_p, Delta_p, tau_ba=tau_dp, delta_beta=Delta_d)

S_h_syn = social_score_iter(tau_h_syn, Delta_h, tau_ba=tau_dh_syn, delta_beta=Delta_d)
S_d_syn = social_score_iter(tau_d_syn, Delta_d, tau_ba=tau_pd_syn, delta_beta=Delta_p)
S_p_syn = social_score_iter(tau_p_syn, Delta_p, tau_ba=tau_dp_syn, delta_beta=Delta_d)

def compare_scores(name, S_real, S_syn, ks):
    rows = metrics(S_real, S_syn, ks)
    df = pd.DataFrame(rows, columns=["k","Precision","Recall","F1","RMSE","MAE","Spearman","Kendall","N"])
    df.insert(0, "Layer", name)
    return df

res_df = pd.concat([
    compare_scores("Hospital(S vs S̃)", S_h_real, S_h_syn, ks=[3,4]),
    compare_scores("Department(S vs S̃)", S_d_real, S_d_syn, ks=[3,4]),
    compare_scores("Doctor(S vs S̃)", S_p_real, S_p_syn, ks=[3,5])
], ignore_index=True)

res_df.to_csv(os.path.join(OUT, "ctgan_socialscore_preservation.csv"), index=False)
print(res_df)

# -----------------------------
# 6) Visualisations
# -----------------------------
for (arr, name) in [
    (tau_h[tau_h>0], "tau_h_real"), (tau_h_syn[tau_h_syn>0], "tau_h_syn"),
    (tau_d[tau_d>0], "tau_d_real"), (tau_d_syn[tau_d_syn>0], "tau_d_syn"),
    (tau_p[tau_p>0], "tau_p_real"), (tau_p_syn[tau_p_syn>0], "tau_p_syn"),
]:
    plot_hist(arr, f"{name} (non-zero)", "trust", os.path.join(OUT, f"{name}_hist.png"))

plot_matrix(tau_h, "τ[h] real", os.path.join(OUT,"tau_h_real.png"))
plot_matrix(tau_h_syn, "τ[h] synthetic", os.path.join(OUT,"tau_h_synth.png"))
plot_matrix(tau_d, "τ[d] real", os.path.join(OUT,"tau_d_real.png"))
plot_matrix(tau_d_syn, "τ[d] synthetic", os.path.join(OUT,"tau_d_synth.png"))
plot_matrix(tau_p, "τ[p] real", os.path.join(OUT,"tau_p_real.png"))
plot_matrix(tau_p_syn, "τ[p] synthetic", os.path.join(OUT,"tau_p_synth.png"))

plot_scatter(normalize(S_h_real), normalize(S_h_syn), "Hospitals: S(real) vs S(synth)", "S real", "S synth", os.path.join(OUT,"scatter_S_h.png"))
plot_scatter(normalize(S_d_real), normalize(S_d_syn), "Departments: S(real) vs S(synth)", "S real", "S synth", os.path.join(OUT,"scatter_S_d.png"))
plot_scatter(normalize(S_p_real), normalize(S_p_syn), "Doctors: S(real) vs S(synth)", "S real", "S synth", os.path.join(OUT,"scatter_S_p.png"))

# Save artifacts
np.save(os.path.join(OUT, "tau_real.npy"), {"h":tau_h,"d":tau_d,"p":tau_p,"hd":tau_hd,"dh":tau_dh,"dp":tau_dp,"pd":tau_pd}, allow_pickle=True)
np.save(os.path.join(OUT, "tau_synth.npy"), {"h":tau_h_syn,"d":tau_d_syn,"p":tau_p_syn,"hd":tau_hd_syn,"dh":tau_dh_syn,"dp":tau_dp_syn,"pd":tau_pd_syn}, allow_pickle=True)

with open(os.path.join(OUT,"data_card.json"), "w") as f:
    json.dump({
        "generator": "CTGAN (SDV)",
        "epochs": 300,
        "notes": "Trained on edge-level table; τ rebuilt with fixed reindexing for shape preservation.",
        "layers": ["H_intra","D_intra","P_intra","HD","DH","DP","PD"]
    }, f, indent=2)

print(f"Artifacts saved under: {OUT}")

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# If needed, run once:
# !pip install sdv==1.13.1 ctgan==0.7.4 pandas numpy matplotlib scipy seaborn

import os, json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr, kendalltau
from sdv.single_table import CTGANSynthesizer
from sdv.metadata import SingleTableMetadata

OUT = "./out_ctgan_trust"
os.makedirs(OUT, exist_ok=True)

# -----------------------------
# Utilities
# -----------------------------
def row_stochastic(M, eps=1e-12):
    M = np.maximum(M, 0.0)
    rs = M.sum(axis=1, keepdims=True)
    rs = np.where(rs < eps, 1.0, rs)
    return M / rs

def normalize(v):
    v = np.asarray(v, dtype=float)
    mn, mx = np.min(v), np.max(v)
    if mx - mn <= 1e-12:
        return np.zeros_like(v)
    return (v - mn) / (mx - mn)

def _safe_rank_corr(a, b):
    if np.std(a) < 1e-12 or np.std(b) < 1e-12:
        return np.nan, np.nan
    rho, _ = spearmanr(a, b)
    tau, _ = kendalltau(a, b)
    return float(rho), float(tau)

def metrics(y_true, y_score, ks):
    r = normalize(y_true); s = normalize(y_score)
    rmse = float(np.sqrt(np.mean((s - r)**2)))
    mae  = float(np.mean(np.abs(s - r)))
    rho, tau = _safe_rank_corr(r, s)

    gt_rank = np.argsort(-r)
    pr_rank = np.argsort(-s)

    rows = []
    for k in ks:
        gt_top = set(gt_rank[:k]); pr_top = set(pr_rank[:k])
        tp = len(gt_top & pr_top)
        prec = tp / k; rec = tp / k
        f1 = 0.0 if prec+rec==0 else 2*prec*rec/(prec+rec)
        rows.append((k, prec, rec, f1, rmse, mae, rho, tau, len(r)))
    return rows

# Damped/PageRank-style propagation to avoid trivial stationary vectors
def social_score_iter(tau_intra, delta_alpha, tau_ba=None, delta_beta=None,
                      damping=0.15, max_iter=200, tol=1e-6, seed=7):
    rng = np.random.default_rng(seed)

    S0 = delta_alpha.copy()
    if (tau_ba is not None) and (delta_beta is not None):
        S0 = S0 + delta_beta @ tau_ba

    # tiny jitter so S0 isn't constant
    S0 = S0 + 1e-6 * rng.normal(size=S0.shape)
    # normalize S0 row-wise
    S0 = S0 / (np.sum(S0, axis=1, keepdims=True) + 1e-12)

    S = S0.copy()
    for _ in range(max_iter):
        S_next = (1.0 - damping) * (S @ tau_intra) + damping * S0
        if np.linalg.norm(S_next - S, ord=1) <= tol:
            S = S_next
            break
        S = S_next
    return S.flatten()

def to_edge_table(tau, layer, src_prefix, dst_prefix):
    n_src, n_dst = tau.shape
    data = []
    for i in range(n_src):
        for j in range(n_dst):
            data.append([layer, f"{src_prefix}{i}", f"{dst_prefix}{j}", float(tau[i,j])])
    return pd.DataFrame(data, columns=["layer","src","dst","trust"])

def filter_layer(df, name, src_prefix=None, dst_prefix=None):
    df_f = df[df["layer"] == name].copy()
    if src_prefix:
        df_f = df_f[df_f["src"].str.startswith(src_prefix)]
    if dst_prefix:
        df_f = df_f[df_f["dst"].str.startswith(dst_prefix)]
    return df_f

# Rebuild synthetic τ with the SAME shape/IDs as the real matrices
def rebuild_tau(df_layer, src_prefix, dst_prefix, all_src_ids=None, all_dst_ids=None):
    df_layer = df_layer[df_layer["src"].str.startswith(src_prefix)]
    df_layer = df_layer[df_layer["dst"].str.startswith(dst_prefix)]
    if all_src_ids is None:
        all_src_ids = sorted(df_layer["src"].unique(), key=lambda x: int(x.replace(src_prefix,"")))
    if all_dst_ids is None:
        all_dst_ids = sorted(df_layer["dst"].unique(), key=lambda x: int(x.replace(dst_prefix,"")))
    piv = df_layer.pivot_table(index="src", columns="dst", values="trust", aggfunc="mean", fill_value=0.0)
    piv = piv.reindex(index=all_src_ids, columns=all_dst_ids, fill_value=0.0)
    return row_stochastic(piv.values.astype(float)), all_src_ids, all_dst_ids

# --- Nice comparative heatmaps (true vs synthetic) ---
def heatmap_pair(true_M, synth_M, title_left, title_right, out_path, vmin=0.0, vmax=1.0, center=0.5):
    fig, axes = plt.subplots(1, 2, figsize=(10, 4), constrained_layout=True)
    sns.heatmap(true_M, ax=axes[0], cmap="RdBu", vmin=vmin, vmax=vmax, center=center, cbar=True)
    axes[0].set_title(title_left)
    sns.heatmap(synth_M, ax=axes[1], cmap="RdBu", vmin=vmin, vmax=vmax, center=center, cbar=True)
    axes[1].set_title(title_right)
    fig.suptitle("True vs Synthetic (CTGAN) trust matrices", y=1.02, fontsize=12)
    plt.savefig(out_path, dpi=220, bbox_inches="tight")
    plt.close(fig)

# -----------------------------
# 1) Example matrices (from your paper snippet)
# -----------------------------
tau_h  = np.array([[0,0.5,0.25,0.25],
                   [0.5,0,0.25,0.25],
                   [0.33,0.33,0,0.33],
                   [0.33,0.33,0.33,0]], dtype=float)

tau_d  = np.array([[0,0.5,0.5,0],
                   [1,0,0,0],
                   [0.5,0,0,0.5],
                   [0,0,1,0]], dtype=float)

tau_p  = np.array([[0,0.17,0.33,0.33,0.16],
                   [0.33,0,0.33,0.33,0],
                   [0.33,0.17,0,0.33,0.16],
                   [0.33,0.17,0.33,0,0.16],
                   [0.33,0,0.33,0.33,0]], dtype=float)

tau_hd = np.array([[0.5,0.25,0.25,0],
                   [0.33,0.67,0,0],
                   [0.67,0,0,0.33],
                   [1,0,0,0]], dtype=float)

tau_dh = np.array([[0.33,0.17,0.33,0.17],
                   [0.33,0.67,0,0],
                   [1,0,0,0],
                   [0,0,1,0]], dtype=float)

tau_dp = np.array([[0.42,0.25,0.33,0,0],
                   [0,0,0.4,0.6,0],
                   [0,0.67,0,0,0.33],
                   [0,0,0,0,1]], dtype=float)

tau_pd = np.array([[1,0,0,0],
                   [0.43,0,0.57,0],
                   [0.67,0.33,0,0],
                   [0,1,0,0],
                   [0,0,0.40,0.60]], dtype=float)

# Non-uniform residuals to avoid constant-vector correlations
rng = np.random.default_rng(7)
Delta_h = (0.2 + 0.02 * rng.normal(size=(1, tau_h.shape[0]))).clip(1e-6, None)
Delta_d = (0.2 + 0.02 * rng.normal(size=(1, tau_d.shape[0]))).clip(1e-6, None)
Delta_p = (0.2 + 0.02 * rng.normal(size=(1, tau_p.shape[0]))).clip(1e-6, None)

# IDs to preserve shapes
ids_h = [f"h{i}" for i in range(tau_h.shape[0])]
ids_d = [f"d{i}" for i in range(tau_d.shape[0])]
ids_p = [f"p{i}" for i in range(tau_p.shape[0])]

# -----------------------------
# 2) Build CTGAN training table
# -----------------------------
edges = []
edges.append(to_edge_table(tau_h, "H_intra", "h", "h"))
edges.append(to_edge_table(tau_d, "D_intra", "d", "d"))
edges.append(to_edge_table(tau_p, "P_intra", "p", "p"))
edges.append(to_edge_table(tau_hd, "HD", "h", "d"))
edges.append(to_edge_table(tau_dh, "DH", "d", "h"))
edges.append(to_edge_table(tau_dp, "DP", "d", "p"))
edges.append(to_edge_table(tau_pd, "PD", "p", "d"))
train_df = pd.concat(edges, ignore_index=True)

meta = SingleTableMetadata()
meta.detect_from_dataframe(train_df)
meta.update_column("layer", sdtype="categorical")
meta.update_column("src", sdtype="categorical")
meta.update_column("dst", sdtype="categorical")
meta.update_column("trust", sdtype="numerical")

# -----------------------------
# 3) Fit CTGAN (batch_size divisible by internal pac)
# -----------------------------
ctgan = CTGANSynthesizer(metadata=meta, epochs=300, batch_size=100, pac=1)  # 100 % 10 == 0
ctgan.fit(train_df)

# -----------------------------
# 4) Sample synthetic edges & rebuild τ with fixed shape
# -----------------------------
synth_df = ctgan.sample(num_rows=len(train_df))

tau_h_syn, _, _  = rebuild_tau(filter_layer(synth_df,"H_intra","h","h"), "h", "h", ids_h, ids_h)
tau_d_syn, _, _  = rebuild_tau(filter_layer(synth_df,"D_intra","d","d"), "d", "d", ids_d, ids_d)
tau_p_syn, _, _  = rebuild_tau(filter_layer(synth_df,"P_intra","p","p"), "p", "p", ids_p, ids_p)
tau_hd_syn,_,_   = rebuild_tau(filter_layer(synth_df,"HD","h","d"), "h", "d", ids_h, ids_d)
tau_dh_syn,_,_   = rebuild_tau(filter_layer(synth_df,"DH","d","h"), "d", "h", ids_d, ids_h)
tau_dp_syn,_,_   = rebuild_tau(filter_layer(synth_df,"DP","d","p"), "d", "p", ids_d, ids_p)
tau_pd_syn,_,_   = rebuild_tau(filter_layer(synth_df,"PD","p","d"), "p", "d", ids_p, ids_d)

# -----------------------------
# 5) Social scores: true vs synthetic
# -----------------------------
S_h_true = social_score_iter(tau_h,  Delta_h, tau_ba=tau_dh, delta_beta=Delta_d, damping=0.15)
S_d_true = social_score_iter(tau_d,  Delta_d, tau_ba=tau_pd, delta_beta=Delta_p, damping=0.15)
S_p_true = social_score_iter(tau_p,  Delta_p, tau_ba=tau_dp, delta_beta=Delta_d, damping=0.15)

S_h_syn  = social_score_iter(tau_h_syn,  Delta_h, tau_ba=tau_dh_syn, delta_beta=Delta_d, damping=0.15)
S_d_syn  = social_score_iter(tau_d_syn,  Delta_d, tau_ba=tau_pd_syn, delta_beta=Delta_p, damping=0.15)
S_p_syn  = social_score_iter(tau_p_syn,  Delta_p, tau_ba=tau_dp_syn, delta_beta=Delta_d, damping=0.15)

def compare_scores(name, y_true, y_syn, ks):
    rows = metrics(y_true, y_syn, ks)
    df = pd.DataFrame(rows, columns=["k","Precision","Recall","F1","RMSE","MAE","Spearman","Kendall","N"])
    df.insert(0, "Layer", name)
    return df

res_df = pd.concat([
    compare_scores("Hospital(S_true vs S_syn)", S_h_true, S_h_syn, ks=[3,4]),
    compare_scores("Department(S_true vs S_syn)", S_d_true, S_d_syn, ks=[3,4]),
    compare_scores("Doctor(S_true vs S_syn)", S_p_true, S_p_syn, ks=[3,5])
], ignore_index=True)

print(res_df)
res_df.to_csv(os.path.join(OUT, "ctgan_socialscore_preservation.csv"), index=False)

# -----------------------------
# 6) Visualizations
# -----------------------------
# Side-by-side heatmaps (true vs synthetic) with seaborn RdBu
heatmap_pair(tau_h,  tau_h_syn,  "τ[h] true",  "τ[h] synthetic",  os.path.join(OUT, "pair_tau_h.png"))
heatmap_pair(tau_d,  tau_d_syn,  "τ[d] true",  "τ[d] synthetic",  os.path.join(OUT, "pair_tau_d.png"))
heatmap_pair(tau_p,  tau_p_syn,  "τ[p] true",  "τ[p] synthetic",  os.path.join(OUT, "pair_tau_p.png"))
heatmap_pair(tau_hd, tau_hd_syn, "τ[hd] true", "τ[hd] synthetic", os.path.join(OUT, "pair_tau_hd.png"))
heatmap_pair(tau_dh, tau_dh_syn, "τ[dh] true", "τ[dh] synthetic", os.path.join(OUT, "pair_tau_dh.png"))
heatmap_pair(tau_dp, tau_dp_syn, "τ[dp] true", "τ[dp] synthetic", os.path.join(OUT, "pair_tau_dp.png"))
heatmap_pair(tau_pd, tau_pd_syn, "τ[pd] true", "τ[pd] synthetic", os.path.join(OUT, "pair_tau_pd.png"))

# Distribution & scatter (quick checks)
def plot_hist(arr, title, xlabel, path):
    plt.figure()
    plt.hist(arr, bins=20)
    plt.title(title); plt.xlabel(xlabel); plt.ylabel("Freq")
    plt.tight_layout(); plt.savefig(path, dpi=200); plt.close()

def plot_scatter(x, y, title, xlabel, ylabel, path):
    plt.figure()
    plt.scatter(x, y, s=20)
    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)
    lo = min(np.min(x), np.min(y)); hi = max(np.max(x), np.max(y))
    plt.plot([lo, hi], [lo, hi], linewidth=1)
    plt.tight_layout(); plt.savefig(path, dpi=200); plt.close()

plot_hist(tau_h[tau_h>0], "τ[h] true (non-zero)", "trust", os.path.join(OUT, "hist_tau_h_true.png"))
plot_hist(tau_h_syn[tau_h_syn>0], "τ[h] synthetic (non-zero)", "trust", os.path.join(OUT, "hist_tau_h_synth.png"))

plot_scatter(normalize(S_h_true), normalize(S_h_syn), "Hospitals: S_true vs S_syn", "S_true (norm)", "S_syn (norm)", os.path.join(OUT,"scatter_S_h.png"))
plot_scatter(normalize(S_d_true), normalize(S_d_syn), "Departments: S_true vs S_syn", "S_true (norm)", "S_syn (norm)", os.path.join(OUT,"scatter_S_d.png"))
plot_scatter(normalize(S_p_true), normalize(S_p_syn), "Doctors: S_true vs S_syn", "S_true (norm)", "S_syn (norm)", os.path.join(OUT,"scatter_S_p.png"))

# Save artifacts
np.save(os.path.join(OUT, "tau_true.npy"), {"h":tau_h,"d":tau_d,"p":tau_p,"hd":tau_hd,"dh":tau_dh,"dp":tau_dp,"pd":tau_pd}, allow_pickle=True)
np.save(os.path.join(OUT, "tau_synth.npy"), {"h":tau_h_syn,"d":tau_d_syn,"p":tau_p_syn,"hd":tau_hd_syn,"dh":tau_dh_syn,"dp":tau_dp_syn,"pd":tau_pd_syn}, allow_pickle=True)

with open(os.path.join(OUT,"data_card.json"), "w") as f:
    json.dump({
        "generator": "CTGAN (SDV)",
        "epochs": 300,
        "batch_size": 100,
        "notes": "Edge-table training; synthetic τ rebuilt with fixed ID reindexing. Damped social-score iteration used.",
        "layers": ["H_intra","D_intra","P_intra","HD","DH","DP","PD"]
    }, f, indent=2)

print(f"Artifacts saved under: {OUT}")

# Visualize TRUE vs SYNTHETIC trust matrices — side-by-side heatmaps (Coolwarm)
# Plus: difference heatmaps, distribution violins, and elementwise scatter plots.
# Assumes you've already run your CTGAN script and saved:
#   ./out_ctgan_trust/tau_true.npy  and  ./out_ctgan_trust/tau_synth.npy

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

OUT = "./out_ctgan_trust"
os.makedirs(OUT, exist_ok=True)

# -----------------------------
# Load artifacts
# -----------------------------
true_pack = np.load(os.path.join(OUT, "tau_true.npy"), allow_pickle=True).item()
synth_pack = np.load(os.path.join(OUT, "tau_synth.npy"), allow_pickle=True).item()

layers = [
    ("h",  "τ[h]"),   # Hospital intra
    ("d",  "τ[d]"),   # Department intra
    ("p",  "τ[p]"),   # Doctor intra
    ("hd", "τ[hd]"),  # Hospital→Department
    ("dh", "τ[dh]"),  # Department→Hospital
    ("dp", "τ[dp]"),  # Department→Doctor
    ("pd", "τ[pd]"),  # Doctor→Department
]

# Seaborn theme
sns.set_context("talk")
sns.set_style("whitegrid")

# -----------------------------
# 1) Side-by-side heatmaps (TRUE vs SYNTH), Coolwarm, no titles
# -----------------------------
fig, axes = plt.subplots(len(layers), 2, figsize=(10, 3*len(layers)), constrained_layout=True)

for row, (key, label) in enumerate(layers):
    T = true_pack[key]
    S = synth_pack[key]

    # Left: TRUE
    sns.heatmap(T, ax=axes[row, 0], cmap="coolwarm", vmin=0.0, vmax=1.0, cbar=True)
    axes[row, 0].set_xlabel("Destination index")
    axes[row, 0].set_ylabel(f"{label} — Source index")

    # Right: SYNTH
    sns.heatmap(S, ax=axes[row, 1], cmap="coolwarm", vmin=0.0, vmax=1.0, cbar=True)
    axes[row, 1].set_xlabel("Destination index")
    axes[row, 1].set_ylabel(f"{label} — Source index")

# No titles requested; just axes labels
plt.savefig(os.path.join(OUT, "heatmaps_true_vs_synth_coolwarm.png"), dpi=220, bbox_inches="tight")
plt.show()

# -----------------------------
# 2) Difference heatmaps (SYNTH - TRUE), centered at 0
# -----------------------------
fig, axes = plt.subplots(len(layers), 1, figsize=(6, 3*len(layers)), constrained_layout=True)

for i, (key, label) in enumerate(layers):
    T = true_pack[key]
    S = synth_pack[key]
    D = S - T
    m = np.max(np.abs(D)) if np.max(np.abs(D)) > 0 else 1.0  # symmetric scale

    ax = axes[i] if len(layers) > 1 else axes
    sns.heatmap(D, ax=ax, cmap="coolwarm", vmin=-m, vmax=m, center=0.0, cbar=True)
    ax.set_xlabel("Destination index")
    ax.set_ylabel(f"{label} — Source index")
    # no title per your request

plt.savefig(os.path.join(OUT, "heatmaps_diff_synth_minus_true_coolwarm.png"), dpi=220, bbox_inches="tight")
plt.show()

# -----------------------------
# 3) Distribution comparison (violins) of non-zero trust values
# -----------------------------
import pandas as pd

def layer_longframe(pack, tag):
    rows = []
    for key, label in layers:
        M = pack[key].astype(float)
        vals = M[M>0].ravel()
        rows.append(pd.DataFrame({"Layer": label, "Trust": vals, "Which": tag}))
    return pd.concat(rows, ignore_index=True)

df_true = layer_longframe(true_pack, "True")
df_syn  = layer_longframe(synth_pack, "Synthetic")
df_all  = pd.concat([df_true, df_syn], ignore_index=True)

plt.figure(figsize=(10, 6))
sns.violinplot(data=df_all, x="Layer", y="Trust", hue="Which", split=True, inner="quartile", palette="coolwarm")
plt.xlabel("Layer")
plt.ylabel("Non-zero trust values")
plt.legend(title="")
plt.tight_layout()
plt.savefig(os.path.join(OUT, "violins_nonzero_trust_true_vs_synth.png"), dpi=220, bbox_inches="tight")
plt.show()

# -----------------------------
# 4) Elementwise scatter: flatten TRUE vs SYNTH with y=x line
# -----------------------------
def scatter_true_vs_synth(T, S, label, path):
    plt.figure(figsize=(5.5, 5.5))
    sns.scatterplot(x=T.ravel(), y=S.ravel(), s=18, alpha=0.7, edgecolor="none", color=sns.color_palette("coolwarm", 2)[1])
    lo = min(np.min(T), np.min(S)); hi = max(np.max(T), np.max(S))
    plt.plot([lo, hi], [lo, hi], linewidth=1, color="black")
    plt.xlabel("True trust")
    plt.ylabel("Synthetic trust")
    # no title
    plt.tight_layout()
    plt.savefig(path, dpi=220, bbox_inches="tight")
    plt.show()

for key, label in layers:
    scatter_true_vs_synth(true_pack[key], synth_pack[key],
                          label,
                          os.path.join(OUT, f"scatter_true_vs_synth_{key}.png"))

# -----------------------------
# (Optional) 5) Correlation barplot per layer (Pearson on flattened entries)
# -----------------------------
corr_rows = []
for key, label in layers:
    T = true_pack[key].ravel().astype(float)
    S = synth_pack[key].ravel().astype(float)
    # pearson (on raw values) + spearman (ranks)
    if np.std(T) < 1e-12 or np.std(S) < 1e-12:
        pear = np.nan; spea = np.nan
    else:
        pear = np.corrcoef(T, S)[0,1]
        from scipy.stats import spearmanr
        spea, _ = spearmanr(T, S)
    corr_rows.append((label, pear, spea))

corr_df = pd.DataFrame(corr_rows, columns=["Layer", "Pearson", "Spearman"])

plt.figure(figsize=(8, 5))
corr_long = corr_df.melt(id_vars="Layer", value_vars=["Pearson","Spearman"], var_name="Metric", value_name="Correlation")
sns.barplot(data=corr_long, x="Layer", y="Correlation", hue="Metric", palette="coolwarm")
plt.ylim(-1.0, 1.0)
plt.xlabel("Layer")
plt.ylabel("Correlation (flattened τ)")
plt.legend(title="")
plt.tight_layout()
plt.savefig(os.path.join(OUT, "barplot_corr_true_vs_synth.png"), dpi=220, bbox_inches="tight")
plt.show()

# Pretty names for subplot titles (to match the screenshot style)
pretty = {
    "h":  "Hospital Intra",
    "d":  "Department Intra",
    "p":  "Doctor Intra",
    "hd": "Hospital→Dept",
    "dh": "Dept→Hospital",
    "dp": "Dept→Doctor",
    "pd": "Doctor→Dept",
}

# Compute symmetric range for cubic differences across all layers
dmax = 1e-9
diff_cubics = {}
for key, _ in layers:
    T = true_pack[key].astype(float)
    S = synth_pack[key].astype(float)
    D_cubic = (S - T)**3
    diff_cubics[key] = D_cubic
    dmax = max(dmax, float(np.nanmax(np.abs(D_cubic))))
norm_diff = TwoSlopeNorm(vmin=-dmax, vcenter=0.0, vmax=dmax)

# Build 3x7 grid: columns = [True^3, Synth^3, (Synth-True)^3]
n_layers = len(layers)
fig, axes = plt.subplots(n_layers, 3, figsize=(13, 3.6*n_layers), constrained_layout=True)

for r, (key, _) in enumerate(layers):
    # Data (cubic transform)
    T = (true_pack[key].astype(float))**3
    S = (synth_pack[key].astype(float))**3
    D = diff_cubics[key]

    # TRUE column
    ax = axes[r, 0]
    im0 = ax.imshow(T, cmap=CMAP, vmin=0.0, vmax=1.0,
                    interpolation=INTERP, origin="upper", aspect="auto")
    ax.set_title(f"{pretty[key]} (True)")
    ax.set_xlabel("Target node index")
    ax.set_ylabel("Source node index")
    ax.grid(False)
    fig.colorbar(im0, ax=ax, fraction=0.03, pad=0.04)

    # SYNTH column
    ax = axes[r, 1]
    im1 = ax.imshow(S, cmap=CMAP, vmin=0.0, vmax=1.0,
                    interpolation=INTERP, origin="upper", aspect="auto")
    ax.set_title(f"{pretty[key]} (Synthetic)")
    ax.set_xlabel("Target node index")
    ax.set_ylabel("Source node index")
    ax.grid(False)
    fig.colorbar(im1, ax=ax, fraction=0.03, pad=0.04)

    # DIFF column
    ax = axes[r, 2]
    im2 = ax.imshow(D, cmap=CMAP, norm=norm_diff,
                    interpolation=INTERP, origin="upper", aspect="auto")
    ax.set_title(f"{pretty[key]} (True - Synthetic)")
    ax.set_xlabel("Target node index")
    ax.set_ylabel("Source node index")
    ax.grid(False)
    cbar = fig.colorbar(im2, ax=ax, fraction=0.03, pad=0.04)
    cbar.set_label("Synthetic − True (cubic)")

# Save
plt.savefig(os.path.join(OUT, "grid_3x7_true_synth_diff_cubic.png"),
            dpi=220, bbox_inches="tight")
plt.show()

# If you don't already have df from earlier, uncomment this block:
# vals_true  = np.concatenate([true_pack[k].astype(float).ravel()  for k, _ in layers])
# vals_synth = np.concatenate([synth_pack[k].astype(float).ravel() for k, _ in layers])
# df = pd.DataFrame({
#     "trust": np.concatenate([vals_true, vals_synth]),
#     "type":  (["True"] * len(vals_true)) + (["Synthetic"] * len(vals_synth)),
# })

# Paired (dodge) bars, log y-axis
sns.set_style("white")
sns.set_context("talk")

fig, ax = plt.subplots(figsize=(9, 5))
bins = np.linspace(0, 1, 21)  # 0..1 in steps of 0.05

sns.histplot(
    data=df, x="trust", hue="type",
    bins=bins, stat="count", multiple="dodge", common_bins=True,
    shrink=0.88, edgecolor=None, alpha=0.9, ax=ax
)

ax.set_yscale("log")
ax.set_xlabel("trust")
ax.set_ylabel("Count")
ax.set_xticks(np.linspace(0, 1, 11))
ax.grid(False)
sns.despine(ax=ax)

plt.tight_layout()
plt.savefig(os.path.join(OUT, "hist_true_vs_synth_log_paired.png"),
            dpi=220, bbox_inches="tight")
plt.show()